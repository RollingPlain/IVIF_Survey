# IVIF Survey: DacTa
Infrared and Visible Image Fusion: From Data Compatibility to Task Adaption
# 我的项目

## 访问统计
![点击统计](https://img.shields.io/badge/dynamic/json?label=访问次数&query=value&url=https://api.countapi.xyz/hit/RollingPlain/IVIF_Survey)

## Star 数量
![GitHub stars](https://img.shields.io/github/stars/RollingPlain/IVIF_Survey?style=social)


# Available Data
# 红外和可见光图像融合(Infrared and visible image fusion)
<table>
    <thead>
        <tr>
            <th>Aspects</th>
            <th>Methods</th>
            <th>Caption</th>
            <th>Venue</th>
            <th>Source</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Auto-Encoder</td>
            <td>DenseFuse</td>
            <td>Densefuse: A fusion approach to infrared and visible images</td>
            <td>TIP '18</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/8580578/">Paper</a>/<a href="https://github.com/hli1221/imagefusion_densefuse">Code</a></td>
        </tr>
        <tr>
            <td>Auto-Encoder</td>
            <td>SEDRFuse</td>
            <td>Sedrfuse: A symmetric encoder–decoder with residual block network for infrared and visible image fusion</td>
            <td>TIM '20</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9187663/">Paper</a>/<a href="https://github.com/jianlihua123/SEDRFuse">Code</a></td>
        </tr>
        <tr>
            <td>Auto-Encoder</td>
            <td>DIDFuse</td>
            <td>Didfuse: Deep image decomposition for infrared and visible image fusion</td>
            <td>IJCAI '20</td>
            <td><a href="https://arxiv.org/abs/2003.09210">Paper</a>/<a href="https://github.com/Zhaozixiang1228/IVIF-DIDFuse">Code</a></td>
        </tr>
        <tr>
            <td>Auto-Encoder</td>
            <td>MFEIF</td>
            <td>Learning a deep multi-scale feature ensemble and an edge-attention guidance for image fusion</td>
            <td>TCSVT '21</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9349250/">Paper</a>/<a href="https://github.com/JinyuanLiu-CV/MFEIF">Code</a></td>
        </tr>
        <tr>
            <td>Auto-Encoder</td>
            <td>RFN-Nest</td>
            <td>Rfn-nest: An end-to-end residual fusion network for infrared and visible images</td>
            <td>TIM '21</td>
            <td><a href="https://www.sciencedirect.com/science/article/pii/S1566253521000440">Paper</a>/<a href="https://github.com/hli1221/imagefusion-rfn-nest">Code</a></td>
        </tr>
        <tr>
            <td>Auto-Encoder</td>
            <td>SFAFuse</td>
            <td>Self-supervised feature adaption for infrared and visible image fusion</td>
            <td>InfFus '21</td>
            <td><a href="https://www.sciencedirect.com/science/article/pii/S1566253521001287">Paper</a>/<a href="https://github.com/zhoafan/SFA-Fuse">Code</a></td>
        </tr>
        <tr>
            <td>Auto-Encoder</td>
            <td>SMoA</td>
            <td>Smoa: Searching a modality-oriented architecture for infrared and visible image fusion</td>
            <td>SPL '21</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9528046/">Paper</a>/<a href="https://github.com/JinyuanLiu-CV/SMoA">Code</a></td>
        </tr>
        <tr>
            <td>Auto-Encoder</td>
            <td>Re2Fusion</td>
            <td>Res2fusion: Infrared and visible image fusion based on dense res2net and double nonlocal attention models</td>
            <td>TIM '22</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9670874/">Paper</a>/<a href="https://github.com/Zhishe-Wang/Res2Fusion">Code</a></td>
        </tr>
        <tr>
            <td>GAN</td>
            <td>FusionGAN</td>
            <td>Fusiongan: A generative adversarial network for infrared and visible image fusion</td>
            <td>InfFus '19</td>
            <td><a href="https://www.sciencedirect.com/science/article/pii/S1566253518301143">Paper</a>/<a href="https://github.com/jiayi-ma/FusionGAN">Code</a></td>
        </tr>
        <tr>
            <td>GAN</td>
            <td>DDcGAN</td>
            <td>Learning a generative model for fusing infrared and visible images via conditional generative adversarial network with dual discriminators</td>
            <td>TIP '19</td>
            <td><a href="https://www.ijcai.org/proceedings/2019/0549.pdf">Paper</a>/<a href="https://github.com/hanna-xu/DDcGAN">Code</a></td>
        </tr>
        <tr>
            <td>GAN</td>
            <td>AtFGAN</td>
            <td>Attentionfgan: Infrared and visible image fusion using attention-based generative adversarial networks</td>
            <td>TMM '20</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9103116">Paper</a></td>
        </tr>
        <tr>
            <td>GAN</td>
            <td>DPAL</td>
            <td>Infrared and visible image fusion via detail preserving adversarial learning</td>
            <td>InfFus '20</td>
            <td><a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253519300314">Paper</a>/<a href="https://github.com/StaRainJ/ResNetFusion">Code</a></td>
        </tr>
        <tr>
            <td>GAN</td>
            <td>D2WGAN</td>
            <td>Infrared and visible image fusion using dual discriminators generative adversarial networks with wasserstein distance</td>
            <td>InfSci '20</td>
            <td><a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025520303431">Paper</a></td>
        </tr>
        <tr>
            <td>GAN</td>
            <td>GANMcC</td>
            <td>Ganmcc: A generative adversarial network with multiclassification constraints for infrared and visible image fusion</td>
            <td>TIM '20</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9274337/">Paper</a>/<a href="https://github.com/HaoZhang1018/GANMcC">Code</a></td>
        </tr>
        <tr>
            <td>GAN</td>
            <td>ICAFusion</td>
            <td>Infrared and visible image fusion via interactive compensatory attention adversarial learning</td>
            <td>TMM '22</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9982426/">Paper</a>/<a href="https://github.com/Zhishe-Wang/ICAFusion">Code</a></td>
        </tr>
        <tr>
            <td>GAN</td>
            <td>TCGAN</td>
            <td>Transformer based conditional gan for multimodal image fusion</td>
            <td>TMM '23</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/10041783/">Paper</a>/<a href="https://github.com/jinxiqinghuan/TCGAN">Code</a></td>
        </tr>
        <tr>
            <td>GAN</td>
            <td>FreqGAN</td>
            <td>Freqgan: Infrared and visible image fusion via unified frequency adversarial learning</td>
            <td>TCSVT '24</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/10680110/">Paper</a>/<a href="https://github.com/Zhishe-Wang/FreqGAN">Code</a></td>
        </tr>
        <tr>
            <td>CNN</td>
            <td>BIMDL</td>
            <td>A bilevel integrated model with data-driven layer ensemble for multi-modality image fusion</td>
            <td>TIP '20</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9293146">Paper</a></td>
        </tr>
        <tr>
            <td>CNN</td>
            <td>MgAN-Fuse</td>
            <td>Multigrained attention network for infrared and visible image fusion</td>
            <td>TIM '20</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9216075">Paper</a></td>
        </tr>
        <tr>
            <td>CNN</td>
            <td>AUIF</td>
            <td>Efficient and model-based infrared and visible image fusion via algorithm unrolling</td>
            <td>TCSVT '21</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9416456">Paper</a>/<a href="https://github.com/Zhaozixiang1228/IVIF-AUIF-Net">Code</a></td>
        </tr>
        <tr>
            <td>CNN</td>
            <td>RXDNFuse</td>
            <td>Rxdnfuse: A aggregated residual dense network for infrared and visible image fusion</td>
            <td>InfFus '21</td>
            <td><a href="https://www.sciencedirect.com/science/article/pii/S1566253520304152">Paper</a></td>
        </tr>
        <tr>
            <td>CNN</td>
            <td>STDFusionNet</td>
            <td>Stdfusionnet: An infrared and visible image fusion network based on salient target detection</td>
            <td>TIM '21</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9416507">Paper</a>/<a href="https://github.com/jiayi-ma/STDFusionNet">Code</a></td>
        </tr>
        <tr>
            <td>CNN</td>
            <td>CUFD</td>
            <td>Cufd: An encoder–decoder network for visible and infrared image fusion based on common and unique feature decomposition</td>
            <td>CVIU '22</td>
            <td><a href="https://www.sciencedirect.com/science/article/abs/pii/S1077314222000352">Paper</a>/<a href="https://github.com/Meiqi-Gong/CUFD">Code</a></td>
        </tr>
        <tr>
            <td>CNN</td>
            <td>Dif-Fusion</td>
            <td>Dif-fusion: Towards high color fidelity in infrared and visible image fusion with diffusion models</td>
            <td>TIP '23</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/10286359/">Paper</a>/<a href="https://github.com/GeoVectorMatrix/Dif-Fusion">Code</a></td>
        </tr>
        <tr>
            <td>CNN</td>
            <td>IGNet</td>
            <td>Learning a graph neural network with cross modality interaction for image fusion</td>
            <td>ACMMM '23</td>
            <td><a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612135">Paper</a>/<a href="https://github.com/lok-18/IGNet">Code</a></td>
        </tr>
        <tr>
            <td>CNN</td>
            <td>LRRNet</td>
            <td>Lrrnet: A novel representation learning guided fusion network for infrared and visible images</td>
            <td>TPAMI '23</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/10105495/">Paper</a>/<a href="https://github.com/hli1221/imagefusion-LRRNet">Code</a></td>
        </tr>
        <tr>
            <td>CNN</td>
            <td>MetaFusion</td>
            <td>Metafusion: Infrared and visible image fusion via meta-feature embedding from object detection</td>
            <td>CVPR '23</td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_MetaFusion_Infrared_and_Visible_Image_Fusion_via_Meta-Feature_Embedding_From_CVPR_2023_paper.html">Paper</a>/<a href="https://github.com/wdzhao123/MetaFusion">Code</a></td>
        </tr>
        <tr>
            <td>CNN</td>
            <td>PSFusion</td>
            <td>Rethinking the necessity of image fusion in high-level vision tasks: A practical infrared and visible image fusion network based on progressive semantic injection and scene fidelity</td>
            <td>InfFus '23</td>
            <td><a href="https://www.sciencedirect.com/science/article/pii/S1566253523001860">Paper</a>/<a href="https://github.com/Linfeng-Tang/PSFusion">Code</a></td>
        </tr>
        <tr>
            <td>Transformer</td>
            <td>SwinFusion</td>
            <td>Swinfusion: Cross-domain long-range learning for general image fusion via swin transformer</td>
            <td>JAS '22</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9812535">Paper</a>/<a href="https://github.com/Linfeng-Tang/SwinFusion">Code</a></td>
        </tr>
        <tr>
            <td>Transformer</td>
            <td>YDTR</td>
            <td>Ydtr: Infrared and visible image fusion via y-shape dynamic transformer</td>
            <td>TMM '22</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9834137">Paper</a>/<a href="https://github.com/tthinking/YDTR">Code</a></td>
        </tr>
        <tr>
            <td>Transformer</td>
            <td>IFT</td>
            <td>Image fusion transformer</td>
            <td>ICIP '22</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/9897280">Paper</a>/<a href="https://github.com/Vibashan/Image-Fusion-Transformer">Code</a></td>
        </tr>
        <tr>
            <td>Transformer</td>
            <td>CDDFuse</td>
            <td>Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion</td>
            <td>CVPR '23</td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html">Paper</a>/<a href="https://github.com/Zhaozixiang1228/MMIF-CDDFuse">Code</a></td>
        </tr>
        <tr>
            <td>Transformer</td>
            <td>TGFuse</td>
            <td>Tgfuse: An infrared and visible image fusion approach based on transformer and generative adversarial network</td>
            <td>TIP '23</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/10122870">Paper</a>/<a href="https://github.com/dongyuya/TGFuse">Code</a></td>
        </tr>
        <tr>
            <td>Transformer</td>
            <td>CMTFusion</td>
            <td>Cross-modal transformers for infrared and visible image fusion</td>
            <td>TCSVT '23</td>
            <td><a href="https://ieeexplore.ieee.org/abstract/document/10163247">Paper</a>/<a href="https://github.com/seonghyun0108/CMTFusion">Code</a></td>
        </tr>
        <tr>
            <td>Transformer</td>
            <td>Text-IF</td>
            <td>Text-if: Leveraging semantic text guidance for degradation-aware and interactive image fusion</td>
            <td>CVPR '24</td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yi_Text-IF_Leveraging_Semantic_Text_Guidance_for_Degradation-Aware_and_Interactive_Image_CVPR_2024_paper.html">Paper</a>/<a href="https://github.com/XunpengYi/Text-IF">Code</a></td>
        </tr>
        <tr>
            <td>Transformer</td>
            <td>PromptF</td>
            <td>Promptfusion: Harmonized semantic prompt learning for infrared and visible image fusion</td>
            <td>JAS '24</td>
            <td></td>
        </tr>
    </tbody>
</table>
